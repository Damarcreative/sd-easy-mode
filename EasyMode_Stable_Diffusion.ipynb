{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Q7lggNI5DEPX",
        "outputId": "4e4d0b85-2a78-45e4-9eaa-33b7ae04921f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "  li img, p img {vertical-align: middle;}\n",
              "  .overview {font-size: 16px;}\n",
              "  .markdown :not(pre)>code {\n",
              "    background-color: #1e1e1e;\n",
              "    border-radius: 2px;\n",
              "    padding: 0 3px;\n",
              "  }\n",
              "  .markdown code {\n",
              "      font-size: 90%;\n",
              "  }\n",
              "</style>\n",
              "<body>\n",
              "<div class=\"markdown overview\">\n",
              "<p>With this Google Colab, you can train an AI text-to-image generator called <a href=\"https://en.wikipedia.org/wiki/Stable_Diffusion\" target=\"_blank\" rel=\"nofollow\">Stable Diffusion</a> to generate images that resemble the photos you provide as input</p>\n",
              "<p>To run a step, press the  <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\"> and wait for <img src=\"https://user-images.githubusercontent.com/507464/210198411-9cb5fdb2-06cf-46fb-9b51-69edd7bab7c6.png\" alt=\"play-loading\">  to finish. You will see a  <img src=\"https://user-images.githubusercontent.com/507464/210198437-7e9737c6-a178-40a9-9b92-0d64113adf89.png\" alt=\"colab-check\"> on the left side of <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\"> when it is complete. Proceed to the next step. Steps 1-3 must be completed before using steps 4-5</p>\n",
              "<ol>\n",
              "<li>Setup - Press <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\"> </li>\n",
              "<li>Upload - Press <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\"> then <img src=\"https://user-images.githubusercontent.com/507464/210199321-28a6c380-1044-423f-a111-3ed96e5a8eb1.png\" alt=\"choose-files\"> will show. Start uploading your photos.</li>\n",
              "<li>Train - Press <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\">. This will take around ~30 minutes to complete.</li>\n",
              "<li>Generate - Change <code>PROMPT</code> and other desired settings then press <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\">. You can repeat this step as many times as you want without rerunning steps 1-3</li>\n",
              "<li style=\"\n",
              "\">Save -  Press <img src=\"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\">. This will save model to Google Drive, you must have at least 2GB free space to continue.</li>\n",
              "</ol>\n",
              "<p>This Colab is based on <a href=\"https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\" target=\"_blank\" rel=\"nofollow\">ShivamShrirao's repository</a> and has been modified by <a href=\"https://github.com/geocine/sd-easy-mode\" target=\"_blank\" rel=\"nofollow\">geocine</a> to be more accessible to complete beginners. It is not intended for advanced or long-term use.</p>\n",
              "</span></div>\n",
              "</body>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @markdown # Overview\n",
        "% % html\n",
        "<style >\n",
        "li img, p img {vertical-align: middle\n",
        "               }\n",
        ".overview {font-size: 16px\n",
        "           }\n",
        ".markdown: not (pre) > code {\n",
        "    background-color:  # 1e1e1e;\n",
        "    border-radius: 2px\n",
        "    padding: 0 3px\n",
        "}\n",
        ".markdown code {\n",
        "    font-size: 90 %\n",
        "}\n",
        "</style >\n",
        "<body >\n",
        "<div class = \"markdown overview\" >\n",
        "<p > With this Google Colab, you can train an AI text-to-image generator called < a href = \"https://en.wikipedia.org/wiki/Stable_Diffusion\" target = \"_blank\" rel = \"nofollow\" > Stable Diffusion < /a > to generate images that resemble the photos you provide as input < /p >\n",
        "<p > To run a step, press the < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > and wait for < img src = \"https://user-images.githubusercontent.com/507464/210198411-9cb5fdb2-06cf-46fb-9b51-69edd7bab7c6.png\" alt = \"play-loading\" > to finish. You will see a < img src = \"https://user-images.githubusercontent.com/507464/210198437-7e9737c6-a178-40a9-9b92-0d64113adf89.png\" alt = \"colab-check\" > on the left side of < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > when it is complete. Proceed to the next step. Steps 1-3 must be completed before using steps 4-5 < /p >\n",
        "<ol >\n",
        "<li > Setup - Press < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > </li >\n",
        "<li > Upload - Press < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > then < img src = \"https://user-images.githubusercontent.com/507464/210199321-28a6c380-1044-423f-a111-3ed96e5a8eb1.png\" alt = \"choose-files\" > will show. Start uploading your photos. < /li >\n",
        "<li > Train - Press < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > . This will take around ~30 minutes to complete. < /li >\n",
        "<li > Generate - Change < code > PROMPT < /code > and other desired settings then press < img src = \"https://user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt = \"play\" > . You can repeat this step as many times as you want without rerunning steps 1-3 < /li >\n",
        "<li style =\"\n",
        "\">Save -  Press <img src=\"https: // user-images.githubusercontent.com/507464/210198375-c6a62f89-cbe4-42f5-9b25-d1f57ca7b6ac.png\" alt=\"play\"> . This will save model to Google Drive, you must have at least 2GB free space to continue . < /li >\n",
        "</ol >\n",
        "<p > This Colab is based on < a href = \"https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\" target = \"_blank\" rel = \"nofollow\" > ShivamShrirao's repository < /a > and has been modified by < a href = \"https://github.com/geocine/sd-easy-mode\" target = \"_blank\" rel = \"nofollow\" > geocine < /a > to be more accessible to complete beginners. It is not intended for advanced or long-term use. < /p >\n",
        "</span > </div >\n",
        "</body >\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "BRANCH = \"exp\"\n",
        "\n",
        "!wget - q - O easymode.py https://github.com/geocine/sd-easy-mode/raw/{BRANCH}/easymode.py\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "from google.colab import output\n",
        "import warnings\n",
        "import subprocess\n",
        "import time\n",
        "from easymode import ProgressBar, install_package, replace_tokens, download_regularization, print_message\n",
        "\n",
        "# Disable the warning message\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
        "                        module=\"IPython.core.interactiveshell\")\n",
        "\n",
        "# *--GPU Check\n",
        "\n",
        "# @markdown During the setup process, the system checks for a graphics processing unit (GPU) with enough memory to run the model. It also installs any necessary Python packages and downloads images that are used to help the model learn more effectively.\n",
        "# Run the nvidia-smi command to get the VRAM information\n",
        "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\",\n",
        "                        \"--format=csv,noheader\"], capture_output=True, check=True)\n",
        "\n",
        "# Split the output by newline characters to get a list of VRAM info for each GPU\n",
        "vram_info = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")\n",
        "\n",
        "# Parse the VRAM info for each GPU\n",
        "for info in vram_info:\n",
        "    name, total, free = info.split(\",\")\n",
        "    total = int(total.strip().split()[0])  # Total VRAM in MB\n",
        "    free = int(free.strip().split()[0])  # Free VRAM in MB\n",
        "\n",
        "    print(f\"GPU: {name}, Total VRAM: {total} MB, Free VRAM: {free} MB\")\n",
        "\n",
        "if total < 15109:  # 15109MB is equivalent to 15GB\n",
        "    # Display an error message in red text\n",
        "    print(\"\\033[91mError: Not enough VRAM available. Please change the runtime to a GPU with at least 15GB VRAM.\\033[0m\")\n",
        "    raise SystemExit\n",
        "else:\n",
        "    print(\"\\033[92mYou have enough VRAM to continue\\033[0m\")\n",
        "\n",
        "# *--Installation\n",
        "\n",
        "!wget - q - O train_dreambooth.py https://github.com/geocine/sd-easy-mode/raw/{BRANCH}/train_dreambooth.py\n",
        "!wget - q - O convert_diffusers_to_original_stable_diffusion.py https://github.com/geocine/sd-easy-mode/raw/{BRANCH}/convert_diffusers_to_original_stable_diffusion.py\n",
        "!wget - q - O concepts_list.json https://github.com/geocine/sd-easy-mode/raw/{BRANCH}/concepts_list.json\n",
        "\n",
        "# URLs of the diffusers and xformers packages\n",
        "DIFFUSERS_URL = 'git+https://github.com/ShivamShrirao/diffusers'\n",
        "XFORMERS_URL = 'https://github.com/geocine/dreamstall-binaries/releases/download/cxx-p38-txx-linux/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl'\n",
        "\n",
        "# List of packages to check and install\n",
        "packages = ['diffusers', 'triton', 'accelerate==0.12.0',\n",
        "            'transformers', 'ftfy', 'bitsandbytes', 'xformers']\n",
        "\n",
        "# Check and install each package\n",
        "pb = ProgressBar(len(packages), \"Installing\")\n",
        "for package in packages:\n",
        "    label = install_package(package, DIFFUSERS_URL, XFORMERS_URL, \"exp\", False)\n",
        "    pb.update(label)\n",
        "print(\"\\033[92mInstallation complete\\033[0m\")\n",
        "\n",
        "# *--Settings\n",
        "\n",
        "SDD_TOKEN = \"zwx\"\n",
        "SDD_CLASS = \"person\"\n",
        "\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "!mkdir - p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\"\n",
        "\n",
        "# check if HUGGINGFACE_TOKEN is set\n",
        "if not HUGGINGFACE_TOKEN:\n",
        "    # Display an error message in red text\n",
        "    print_message(\"warning\", \"Please set HUGGINGFACE_TOKEN first.\")\n",
        "\n",
        "!echo - n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
        "\n",
        "OUTPUT_DIR = f\"stable_diffusion_models/{SDD_TOKEN}\"\n",
        "OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    # Change to the directory\n",
        "    os.chdir(OUTPUT_DIR)\n",
        "    # Remove all files and directories inside the directory using the rm command\n",
        "    subprocess.run([\"rm\", \"-rf\", \"*\"], check=True)\n",
        "else:\n",
        "    # Create the directory\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "print(f\"[*] Models will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "regularizations = [\"person\"]\n",
        "\n",
        "for regularization in regularizations:\n",
        "    download_regularization(regularization)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0W530tWf904D"
      },
      "source": [
        "# Upload\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "32gYIDDR1aCp"
      },
      "outputs": [],
      "source": [
        "# @markdown To train the model, run this cell to upload 15-20 images of your subject. The images should be 512x512 in size and show the subject in various poses, expressions, and backgrounds. The images should show the subject in different variations. If your images are not already 512x512, you can use [this tool](https://www.birme.net/?target_width=512&target_height=512) to resize them in a batch.\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# check if /content/concepts_list.json exists if not remind to run install\n",
        "if not os.path.exists(\"/content/concepts_list.json\"):\n",
        "    print_message(\"warning\", \"Please run the Setup cell first\")\n",
        "\n",
        "replace_tokens(\"/content/concepts_list.json\", SDD_TOKEN)\n",
        "\n",
        "# Load the data from the JSON file into the concepts_list variable\n",
        "with open(\"/content/concepts_list.json\", \"r\") as f:\n",
        "    concepts_list = json.load(f)\n",
        "\n",
        "# Incorporate this so that users won't have to crop their images https://github.com/d8ahazard/sd_smartprocess\n",
        "for c in concepts_list:\n",
        "    prompt = c['instance_prompt']\n",
        "    prompt = prompt.format(SDD_TOKEN=SDD_TOKEN, SDD_CLASS=SDD_CLASS)\n",
        "    print(f\"Uploading instance images for `{prompt}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        # Create the instance_data_dir directory if it does not exist\n",
        "        os.makedirs(c['instance_data_dir'], exist_ok=True)\n",
        "        shutil.move(filename, dst_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jjcSXTp-u-Eh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from easymode import create_interpolation_function\n",
        "\n",
        "# @markdown The number of steps is currently set to auto-compute(-1), but you can adjust them to try and improve accuracy. More steps usually improve accuracy, but too many can cause the model to overfit and perform poorly on other tasks like styling. Fewer steps may result in less accurate models. Only change these settings if you understand the potential consequences and are familiar with the process. If you do adjust the number of steps, make small changes and test the model's performance.\n",
        "\n",
        "# Load the data from the JSON file into the concepts_list variable\n",
        "with open(\"/content/concepts_list.json\", \"r\") as f:\n",
        "    concepts_list = json.load(f)\n",
        "\n",
        "# Counter for the total number of images\n",
        "num_images = 0\n",
        "\n",
        "for c in concepts_list:\n",
        "    data_dir = c['instance_data_dir']\n",
        "    # replace the SDD_TOKEN placeholders with the actual values\n",
        "    data_dir = data_dir.format(SDD_TOKEN=SDD_TOKEN)\n",
        "    # Check if the directory exists\n",
        "    if os.path.exists(data_dir):\n",
        "        # Check if the directory is empty\n",
        "        num_files = len(os.listdir(data_dir))\n",
        "        if num_files == 0:\n",
        "            print_message(\"error\", f\"The directory `{data_dir}` is empty. Please upload some images using the Upload step above.\")\n",
        "        else:\n",
        "            num_images += num_files\n",
        "    else:\n",
        "        # Raise an exception if the directory does not exist\n",
        "        print_message(\"error\", f\"The directory `{data_dir}` does not exist. Please run the Upload cell first\")\n",
        "\n",
        "\n",
        "# interpolation computation based on Astria results\n",
        "interpolate_max_train_steps = create_interpolation_function(\n",
        "    [(10, 1611), (11, 1750), (15, 2281)])\n",
        "\n",
        "# compute NUM_CLASS_IMAGES based on the number of images , num_images * 200 with a limit of 4000\n",
        "# min(num_images * 200, 4000)\n",
        "NUM_CLASS_IMAGES = num_images * 10\n",
        "SAVE_SAMPLE_PROMPT = \"photo of {TOKEN_CLASS}\"\n",
        "SAVE_SAMPLE_PROMPT = SAVE_SAMPLE_PROMPT.format(\n",
        "    TOKEN_CLASS=f\"{SDD_TOKEN} {SDD_CLASS}\")\n",
        "MAX_TRAIN_STEPS = -1  # @param {type:\"number\"}\n",
        "if MAX_TRAIN_STEPS < 0:\n",
        "    MAX_TRAIN_STEPS = int(interpolate_max_train_steps(num_images))\n",
        "\n",
        "# @param [\"person\", \"man\", \"woman\", \"dog\", \"cat\", \"artstyle\"]\n",
        "SDD_CLASS = \"person\"\n",
        "# @markdown `SDD_CLASS` is the subject type you want to train.<br><br>\n",
        "# @markdown The default value for `SDD_CLASS` is `person`. For example, if you set `SDD_CLASS` to `dog` then use the prompt `zwx dog` on the **Generate** step.\n",
        "\n",
        "replace_tokens(\"/content/concepts_list.json\", SDD_TOKEN, SDD_CLASS)\n",
        "\n",
        "SAVE_INTERVAL = 10000\n",
        "SAVE_MIN_STEPS = 0\n",
        "CLEAR_MODELS = True\n",
        "SAMPLE_BATCH_SIZE = 4\n",
        "\n",
        "\n",
        "# Check SAVE_MIN_STEPS should be should be less than or equal MAX_TRAIN_STEPS\n",
        "if SAVE_MIN_STEPS > MAX_TRAIN_STEPS:\n",
        "    print_message(\"error\", \"Your model will not be saved if SAVE_MIN_STEPS is greater than MAX_TRAIN_STEPS.\")\n",
        "\n",
        "PREV_MODEL_STEPS = None\n",
        "g_cuda = None\n",
        "\n",
        "if CLEAR_MODELS:\n",
        "    # Run the rm command using subprocess\n",
        "    subprocess.run(\n",
        "        [\"rm\", \"-rf\", f\"/content/stable_diffusion_models/{SDD_TOKEN}/*\"])\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "    --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "    --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "    --output_dir=$OUTPUT_DIR \\\n",
        "    --revision=\"main\" \\\n",
        "    --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "    --seed=1337 \\\n",
        "    --resolution=512 \\\n",
        "    --train_batch_size=1 \\\n",
        "    --train_text_encoder \\\n",
        "    --mixed_precision=\"fp16\" \\\n",
        "    --use_8bit_adam \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --learning_rate=1e-6 \\\n",
        "    --lr_scheduler=\"constant\" \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --num_class_images=$NUM_CLASS_IMAGES \\\n",
        "    --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
        "    --max_train_steps=$MAX_TRAIN_STEPS \\\n",
        "    --save_interval=$SAVE_INTERVAL \\\n",
        "    --save_min_steps=$SAVE_MIN_STEPS \\\n",
        "    --save_sample_prompt=\"$SAVE_SAMPLE_PROMPT\" \\\n",
        "    --concepts_list=\"concepts_list.json\"\n",
        "#   --shuffle_after_epoch\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "# Generate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# @markdown The default value for `SDD_CLASS` is `person`. If you trained a different class, update the prompts accordingly. For example, if you set `SDD_CLASS` to `dog` then replace `zwx {SDD_CLASS}` with `zwx dog`.<br><br>\n",
        "# @markdown To generate images, change the parameters and run the cell. Include `zwx {SDD_CLASS}` in your prompts. For example: `a photo of zwx {SDD_CLASS}`. If you want more prompt ideas, you can check out [Astria's gallery](https://www.astria.ai/gallery) and replace `sks|zwx person|man|woman` with `zwx {SDD_CLASS}`.<br><br>\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "import random\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler, EulerAncestralDiscreteScheduler\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "MODEL_STEPS = MAX_TRAIN_STEPS\n",
        "\n",
        "\n",
        "if \"SDD_TOKEN\" not in globals():\n",
        "    print_message(\"error\", \"Please run the Setup step above.\")\n",
        "\n",
        "if \"PREV_MODEL_STEPS\" not in globals():\n",
        "    print_message(\"error\", \"Please run the Training step above.\")\n",
        "\n",
        "if not os.path.exists(f'/content/stable_diffusion_models/{SDD_TOKEN}/{MODEL_STEPS}'):\n",
        "    print_message(\"error\", f\"Model with {MODEL_STEPS} steps does not exist. Please make sure you have run the Training step above.\")\n",
        "\n",
        "if MODEL_STEPS != PREV_MODEL_STEPS or g_cuda is None:\n",
        "    print(\"Loading model...\")\n",
        "    PREV_MODEL_STEPS = MODEL_STEPS\n",
        "    # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "    model_path = f'/content/stable_diffusion_models/{SDD_TOKEN}/{MODEL_STEPS}'\n",
        "    model_path = model_path.replace(\"{TOKEN}\", SDD_TOKEN)\n",
        "    scheduler = EulerAncestralDiscreteScheduler(\n",
        "        num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    g_cuda = torch.Generator(device='cuda')\n",
        "\n",
        "# Make sure model_path exists\n",
        "if not os.path.exists(model_path):\n",
        "    print_message(\"error\", f\"Model with {MODEL_STEPS} steps does not exist. Please make sure you have run the Training step above.\")\n",
        "\n",
        "SEED = -1\n",
        "if (SEED < 0):\n",
        "    SEED = random.randint(0, 2**32 - 1)\n",
        "g_cuda.manual_seed(SEED)\n",
        "\n",
        "# @param {type:\"string\"}\n",
        "PROMPT = \"closeup photo of zwx person, trending on artstation, by greg rutkowski, alphonse mucha\"\n",
        "PROMPT = PROMPT.format(TOKEN_CLASS=f\"{SDD_TOKEN} {SDD_CLASS}\")\n",
        "# @param {type:\"string\"}\n",
        "NEGATIVE_PROMPT = \"ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, bad anatomy, bad proportions, cloned face, disfigured, out of frame, extra limbs, bad anatomy, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, too many fingers, long neck, text, letters, signature, web address, copyright name, username, error, extra digit, fewer digits, loadscreen, grid, stock image, a stock photo, promo poster, fat\"\n",
        "NUM_IMAGES_PER_PROMPT = 4  # @param {type:\"number\"}\n",
        "GUIDANCE_SCALE = 7.5  # @param {type:\"number\"}\n",
        "INFERENCE_STEPS = 50  # @param {type:\"number\"}\n",
        "height = 512\n",
        "width = 512\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt=PROMPT,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=NEGATIVE_PROMPT,\n",
        "        num_images_per_prompt=NUM_IMAGES_PER_PROMPT,\n",
        "        num_inference_steps=INFERENCE_STEPS,\n",
        "        guidance_scale=GUIDANCE_SCALE,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "text = \"\"\"\n",
        "<br>\n",
        "If you're not happy with the output, you can try adjusting the <code>GUIDANCE_SCALE</code> and <code>INFERENCE_STEPS</code> parameters to improve the accuracy and quality of the generated images. \n",
        "<br>\n",
        "If the model is not generating a likeness, try using higher quality reference photos or increasing the number of training steps, then starting the training process again.\n",
        "<br>\n",
        "<br>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(text))\n",
        "\n",
        "for img in images:\n",
        "    display(img)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3EydphztWBiI"
      },
      "source": [
        "# Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hHvppaFtcFA4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if \"SDD_TOKEN\" not in globals():\n",
        "    print_message(\"error\", \"Please run the Setup step above.\")\n",
        "\n",
        "# @markdown This will save your trained model to your Google Drive. You can then download it and use it offline with the desktop application [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui). Please join [Stable Diffusion Dreambooth Discord](https://discord.com/invite/qbMuXBXyHA), we have a helpful community.\n",
        "MODEL_STEPS = MAX_TRAIN_STEPS\n",
        "mdl_path = f\"/content/stable_diffusion_models/{SDD_TOKEN}/{MODEL_STEPS}\"\n",
        "mdl_path = mdl_path.replace(\"{TOKEN}\", SDD_TOKEN)\n",
        "ckpt_path = mdl_path + \"/model.ckpt\"\n",
        "\n",
        "# Make sure model_path exists\n",
        "if not os.path.exists(mdl_path):\n",
        "    print_message(\"error\", f\"Model with {MODEL_STEPS} steps does not exist. Please make sure you have run the Training step above.\")\n",
        "\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path mdl_path --checkpoint_path $ckpt_path --half\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")\n",
        "\n",
        "# Check if Google Drive is already mounted\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    # Mount Google Drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "NAME = \"me\"  # @param {type:\"string\"}\n",
        "# @markdown Enter the path to save the model in Google Drive. If left empty, the model will be saved in the root of Google Drive.\n",
        "GDRIVE_PATH = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# remove / from start and end of GDRIVE_PATH if they exist\n",
        "GDRIVE_PATH = GDRIVE_PATH.strip('/')\n",
        "MODEL_NAME = f\"{SDD_CLASS}-{MODEL_STEPS}-{SDD_TOKEN}-{NAME}\"\n",
        "if GDRIVE_PATH:\n",
        "    cmd = f\"cp /content/stable_diffusion_models/{SDD_TOKEN}/{MODEL_STEPS}/model.ckpt /content/drive/MyDrive/{GDRIVE_PATH}/{MODEL_NAME}.ckpt\"\n",
        "else:\n",
        "    cmd = f\"cp /content/stable_diffusion_models/{SDD_TOKEN}/{MODEL_STEPS}/model.ckpt /content/drive/MyDrive/{MODEL_NAME}.ckpt\"\n",
        "\n",
        "# Execute the command\n",
        "!{cmd}\n",
        "if GDRIVE_PATH:\n",
        "    print(\n",
        "        f\"Model saved at /{GDRIVE_PATH}/{MODEL_NAME}.ckpt Wait for 5 minutes before closing\")\n",
        "else:\n",
        "    print(\n",
        "        f\"Model saved at /{MODEL_NAME}.ckpt Wait for 5 minutes before closing\")\n",
        "\n",
        "print(\n",
        "    f\"To use your model on other applications make sure to mention \\\"{SDD_TOKEN} {SDD_CLASS}\\\" in the prompt.\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E3L4EhLn4Cb8"
      },
      "source": [
        "Here are some resources you may find helpful as you continue learning about Stable Diffusion Dreambooth:\n",
        "\n",
        "- [The guide to fine-tuning Stable Diffusion with your own images](https://tryolabs.com/blog/2022/10/25/the-guide-to-fine-tuning-stable-diffusion-with-your-own-images)\n",
        "- [Basic Dreambooth Guide](https://github.com/nitrosocke/dreambooth-training-guide)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd7c1d5a5e6e0fb14b3746a29a0c26165797e9d13a4bcff2a622ade35f8f64ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
